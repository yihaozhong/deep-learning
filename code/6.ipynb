{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "\n",
    "Yihao Zhong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model interpretability and explainability are important because they enable trust, transparency, and account for robustness for downstream users. Interpretable models allow downstream users to understand how predictions are made, facilitating debugging, improvement, and also compliance with regulations (equality, non-descriminatic etc). Explainability helps detect biases, uncover insights, and enhance user acceptance. Without interpretability, the decision-making process remains a \"black box,\" hindering trust and limiting the model's practical applicability in domains that require clear explanations, such as healthcare and finance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autofeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autofeat import FeatureSelector, AutoFeatRegressor\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features:  10\n"
     ]
    }
   ],
   "source": [
    "# Load the diabetes dataset and get the featues and target\n",
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "print(\"Original number of features: \", X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[featsel] Scaling data..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 13:37:15,434 INFO: [featsel] Feature selection run 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 13:37:16,154 INFO: [featsel] Feature selection run 2/5\n",
      "2024-04-27 13:37:16,188 INFO: [featsel] Feature selection run 3/5\n",
      "2024-04-27 13:37:16,220 INFO: [featsel] Feature selection run 4/5\n",
      "2024-04-27 13:37:16,251 INFO: [featsel] Feature selection run 5/5\n",
      "2024-04-27 13:37:16,284 INFO: [featsel] 7 features after 5 feature selection runs\n",
      "/Users/zhongyihao/anaconda3/envs/trading/lib/python3.12/site-packages/autofeat/featsel.py:270: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  if np.max(np.abs(correlations[c].ravel()[:i])) < 0.9:\n",
      "2024-04-27 13:37:16,286 INFO: [featsel] 7 features after correlation filtering\n",
      "2024-04-27 13:37:16,295 INFO: [featsel] 6 features after noise filtering\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New number of features:  6\n"
     ]
    }
   ],
   "source": [
    "fsel = FeatureSelector(verbose=1)\n",
    "\n",
    "# Fit it on the train dataset\n",
    "df = fsel.fit_transform(X, y)\n",
    "print(\"New number of features: \", df.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "There are 10 original features, and after features selection there are 6 features, the discarded features is 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on the training set:  0.5279193863361498\n",
      "R2 score on the test set:  0.45260276297191915\n"
     ]
    }
   ],
   "source": [
    "# Perform a train-test split on your dataset. Select a regression model from skLearn and fit it to the\n",
    "# training dataset. What is the R2 score on the training and test set?\n",
    "\n",
    "# Perform a train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rreg_model = LinearRegression()\n",
    "result_logistic = rreg_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"R2 score on the training set: \", result_logistic.score(X_train, y_train))\n",
    "print(\"R2 score on the test set: \", result_logistic.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a linear regression model. The $R^2$ for training set is 0.53, and the $R^2$ for testing set is 0.45. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 13:37:30,791 INFO: [AutoFeat] The 3 step feature engineering process could generate up to 60445 features.\n",
      "2024-04-27 13:37:30,792 INFO: [AutoFeat] With 353 data points this new feature matrix would use about 0.09 gb of space.\n",
      "2024-04-27 13:37:30,796 INFO: [feateng] Step 1: transformation of original features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feateng]               0/             10 features transformed\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 13:37:31,819 INFO: [feateng] Generated 45 transformed features from 10 original features - done.\n",
      "2024-04-27 13:37:31,824 INFO: [feateng] Step 2: first combination of features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feateng]            1400/           1485 feature tuples combined\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 13:37:33,274 INFO: [feateng] Generated 5789 feature combinations from 1485 original feature tuples - done.\n",
      "2024-04-27 13:37:33,279 INFO: [feateng] Step 3: transformation of new features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feateng]            5000/           5789 features transformed\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhongyihao/anaconda3/envs/trading/lib/python3.12/site-packages/numpy/core/_methods.py:176: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "/Users/zhongyihao/anaconda3/envs/trading/lib/python3.12/site-packages/numpy/core/_methods.py:187: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feateng]            5600/           5789 features transformed\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 13:37:43,013 INFO: [feateng] Generated 24430 transformed features from 5789 original features - done.\n",
      "2024-04-27 13:37:43,053 INFO: [feateng] Generated altogether 32266 new features in 3 steps\n",
      "2024-04-27 13:37:43,053 INFO: [feateng] Removing correlated features, as well as additions at the highest level\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feateng]            5700/           5789 features transformed\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 13:37:43,270 INFO: [feateng] Generated a total of 14871 additional features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[featsel] Scaling data..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 13:37:44,076 INFO: [featsel] Feature selection run 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 13:38:28,567 INFO: [featsel] Feature selection run 2/5\n",
      "2024-04-27 13:39:09,195 INFO: [featsel] Feature selection run 3/5\n",
      "2024-04-27 13:39:38,424 INFO: [featsel] Feature selection run 4/5\n",
      "2024-04-27 13:40:33,087 INFO: [featsel] Feature selection run 5/5\n",
      "2024-04-27 13:41:01,473 INFO: [featsel] 36 features after 5 feature selection runs\n",
      "/Users/zhongyihao/anaconda3/envs/trading/lib/python3.12/site-packages/autofeat/featsel.py:270: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  if np.max(np.abs(correlations[c].ravel()[:i])) < 0.9:\n",
      "2024-04-27 13:41:01,487 INFO: [featsel] 31 features after correlation filtering\n",
      "2024-04-27 13:41:01,685 INFO: [featsel] 11 features after noise filtering\n",
      "2024-04-27 13:41:01,705 INFO: [AutoFeat] Computing 11 new features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AutoFeat]     6/   11 new features\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 13:41:02,327 INFO: [AutoFeat]    11/   11 new features ...done.\n",
      "2024-04-27 13:41:02,335 INFO: [AutoFeat] Final dataframe with 21 feature columns (11 new).\n",
      "2024-04-27 13:41:02,335 INFO: [AutoFeat] Training final regression model.\n",
      "2024-04-27 13:41:02,344 INFO: [AutoFeat] Trained model: largest coefficients:\n",
      "2024-04-27 13:41:02,345 INFO: -70.30327923185092\n",
      "2024-04-27 13:41:02,345 INFO: 781094.306607 * x000**3*x001\n",
      "2024-04-27 13:41:02,346 INFO: -3983.086911 * exp(x006)*Abs(x001)\n",
      "2024-04-27 13:41:02,346 INFO: 362.715436 * exp(x002)*exp(x003)\n",
      "2024-04-27 13:41:02,347 INFO: 332.458747 * x000**9/x009**3\n",
      "2024-04-27 13:41:02,347 INFO: 215.883125 * Abs(x002 + Abs(x009))\n",
      "2024-04-27 13:41:02,347 INFO: 37.282823 * exp(x002)*exp(x008)\n",
      "2024-04-27 13:41:02,348 INFO: 24.680193 * Abs(x008)/x008\n",
      "2024-04-27 13:41:02,348 INFO: -4.397543 * x004**2*Abs(1/x003)\n",
      "2024-04-27 13:41:02,348 INFO: -0.854832 * x006/Abs(x002)\n",
      "2024-04-27 13:41:02,349 INFO: 0.037580 * 1/(x003**2 - x008)\n",
      "2024-04-27 13:41:02,349 INFO: -0.010361 * 1/(x008 - Abs(x009))\n",
      "2024-04-27 13:41:02,350 INFO: [AutoFeat] Final score: 0.6100\n",
      "2024-04-27 13:41:02,351 INFO: [AutoFeat] Computing 11 new features.\n",
      "2024-04-27 13:41:02,359 INFO: [AutoFeat]    11/   11 new features ...done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Final Train R^2: 0.6224 features\n",
      "## Final Test R^2: 0.5172\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an AutoFeatRegressor with 3 steps of feature engineering\n",
    "afreg = AutoFeatRegressor(verbose=1, feateng_steps=3)\n",
    "\n",
    "# Fit it on the train dataset\n",
    "X_train_afreg = afreg.fit_transform(X_train, y_train)\n",
    "X_test_afreg = afreg.transform(X_test)\n",
    "\n",
    "result_afreg = rreg_model.fit(X_train_afreg, y_train)\n",
    "\n",
    "print(\"## Final Train R^2: %.4f\" % result_afreg.score(X_train_afreg, y_train))\n",
    "print(\"## Final Test R^2: %.4f\" %  result_afreg.score(X_test_afreg, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Features:  {'x001', 'x004**2*Abs(1/x003)', 'x003', 'x006/Abs(x002)', 'Abs(x008)/x008', 'x004', 'x002', 'exp(x006)*Abs(x001)', 'x000**9/x009**3', 'exp(x002)*exp(x003)', 'Abs(x002 + Abs(x009))', 'x008', 'x009', '1/(x008 - Abs(x009))', 'x005', 'x000', 'x000**3*x001', 'x007', '1/(x003**2 - x008)', 'exp(x002)*exp(x008)', 'x006'}\n",
      "Original Features:  ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n"
     ]
    }
   ],
   "source": [
    "print(\"New Features: \", set(X_train_afreg.columns))\n",
    "print(\"Original Features: \", datasets.load_diabetes().feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ score on the train data is 0.62, increased by 17%. The $R^2$ score on the test data is 0.45, increased by 14.3%. The performance improve after we use the Autofeat, which introduce more features and hence more complexity and non-linearity to the regression model. \n",
    "\n",
    "Five new features are 'x004**2*Abs(1/x003)', 'x006/Abs(x002)', 'Abs(x008)/x008', 'exp(x006)*Abs(x001)', 'exp(x002)*exp(x003)'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets, neighbors, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import uniform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
    "X_digits = X_digits / X_digits.max()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits,\n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(config):\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    drop_rate = config[\"drop_rate\"]\n",
    "    num_classes = 10\n",
    "    epochs = 12\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(filters=config[\"conv_filters\"], kernel_size=(3, 3), \n",
    "                               activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(config[\"hidden\"], activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(drop_rate),\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            lr=config[\"lr\"]),\n",
    "        metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=0,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[TuneReportCallback({\n",
    "            \"mean_accuracy\": \"accuracy\"\n",
    "        })])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "        train_mnist,\n",
    "        name=\"exp\",\n",
    "        metric=\"mean_accuracy\",\n",
    "        mode=\"max\",\n",
    "        stop={\n",
    "            \"mean_accuracy\": 0.99,\n",
    "        },\n",
    "        resources_per_trial={\n",
    "            \"gpu\": 1\n",
    "        },\n",
    "        config={\n",
    "            \"batch_size\": tune.grid_search([32, 64, 128]),\n",
    "            \"hidden\": tune.grid_search([128, 256, 512]),\n",
    "            \"conv_filters\": tune.grid_search([16,32,64]),\n",
    "            \"lr\": tune.uniform(0.001, 0.1),\n",
    "            \"drop_rate\": tune.uniform(0.0, 1.0),\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learner 1 is 2.5x faster than learner 2\n",
    "\n",
    "The staleness as follows:\n",
    "\n",
    "$g[L_1,2]$: 0, this is the start and first of the calculation, of both learner 1 and 2\n",
    "\n",
    "$g[L_1,2]$: 0, still there is no updated gradient from other learner during computation. \n",
    "\n",
    "$g[L_1,3]$: 1, it fail to pick up learner 2 g[L_2, 2] which happen at 2.5s, while learner 1 is running from 2s to 3s.\n",
    "\n",
    "$g[L_1,4]$: 0, there is no updated gradient from other learner during computation. \n",
    "\n",
    "$g[L_2,1]$: 0, this is the start and first of the calculation, of both learner 1 and 2\n",
    "\n",
    "$g[L_2,2]$: 2, there are 2 results from learn 1 updated during 2.5 s to 5 s: $g[L_1,3]$ and $g[L_1,4]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-ssd'...\n",
      "remote: Enumerating objects: 819, done.\u001b[K\n",
      "remote: Counting objects: 100% (437/437), done.\u001b[K\n",
      "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
      "remote: Total 819 (delta 411), reused 405 (delta 405), pack-reused 382\u001b[K\n",
      "Receiving objects: 100% (819/819), 1.05 MiB | 10.35 MiB/s, done.\n",
      "Resolving deltas: 100% (552/552), done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/qfgaohao/pytorch-ssd.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/zhongyihao/anaconda3/envs/torch_nightly_env/lib/python3.10/site-packages (4.9.0.80)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.4 in /Users/zhongyihao/anaconda3/envs/torch_nightly_env/lib/python3.10/site-packages (from opencv-python) (1.24.3)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pytorch-ssd/eval_ssd.py --net mb1-ssd  --dataset data/VOCdevkit/VOC2007 --trained_model pytorch-ssd/models/mobilenet-v1-ssd-mp-0_675.pth --label_file pytorch-ssd/models/voc-model-labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is as follow, (copy and paste from hpc from above command)\n",
    "\n",
    "Average Precision Per-class:\n",
    "\n",
    "aeroplane: 0.6843271224059599\n",
    "\n",
    "bicycle: 0.7911140237662206\n",
    "\n",
    "bird: 0.6171819168583986\n",
    "\n",
    "boat: 0.5612220055063379\n",
    "\n",
    "bottle: 0.3485216621466003\n",
    "\n",
    "bus: 0.7677814849265677\n",
    "\n",
    "car: 0.7280986468467315\n",
    "\n",
    "cat: 0.8369208203985581\n",
    "\n",
    "chair: 0.5169138632991064\n",
    "\n",
    "cow: 0.6238697603075337\n",
    "\n",
    "diningtable: 0.7062172972736019\n",
    "\n",
    "dog: 0.7872868219961326\n",
    "\n",
    "horse: 0.819446325939355\n",
    "\n",
    "motorbike: 0.7918539457195842\n",
    "\n",
    "person: 0.702363739134837\n",
    "\n",
    "pottedplant: 0.39852951468542563\n",
    "\n",
    "sheep: 0.6066678298227772\n",
    "\n",
    "sofa: 0.7573083661544429\n",
    "\n",
    "train: 0.8262441264750008\n",
    "\n",
    "tvmonitor: 0.6461898726506375\n",
    "\n",
    "Average Precision Across All Classes: 0.6759029573156905"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.34.93-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting s3transfer<0.11.0,>=0.10.0\n",
      "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.35.0,>=1.34.93\n",
      "  Downloading botocore-1.34.93-py3-none-any.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/zhongyihao/anaconda3/envs/torch_nightly_env/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.93->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/zhongyihao/anaconda3/envs/torch_nightly_env/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.93->boto3) (1.26.15)\n",
      "Requirement already satisfied: six>=1.5 in /Users/zhongyihao/anaconda3/envs/torch_nightly_env/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.93->boto3) (1.16.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.34.93 botocore-1.34.93 jmespath-1.0.1 s3transfer-0.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-28 10:37:15,892 - root - Download https://storage.googleapis.com/openimages/2018_04/class-descriptions-boxable.csv.\n",
      "2024-04-28 10:37:16,110 - root - Download https://storage.googleapis.com/openimages/2018_04/train/train-annotations-bbox.csv.\n",
      "2024-04-28 10:37:36,477 - root - Read annotation file data/open_images/train-annotations-bbox.csv\n",
      "2024-04-28 10:37:45,615 - root - train bounding boxes size: 1307\n",
      "2024-04-28 10:37:45,615 - root - Approximate Image Stats: \n",
      "/Users/zhongyihao/Downloads/DS301 Advance ML/deep-learning/code/pytorch-ssd/open_images_downloader.py:63: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for k, count in values.value_counts().iteritems():\n",
      "2024-04-28 10:37:45,617 - root - Handgun: 561/990 = 0.57.\n",
      "2024-04-28 10:37:45,617 - root - Shotgun: 429/990 = 0.43.\n",
      "2024-04-28 10:37:45,617 - root - Label distribution: \n",
      "/Users/zhongyihao/Downloads/DS301 Advance ML/deep-learning/code/pytorch-ssd/open_images_downloader.py:63: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for k, count in values.value_counts().iteritems():\n",
      "2024-04-28 10:37:45,617 - root - Handgun: 727/1307 = 0.56.\n",
      "2024-04-28 10:37:45,617 - root - Shotgun: 580/1307 = 0.44.\n",
      "2024-04-28 10:37:45,617 - root - Shuffle dataset.\n",
      "2024-04-28 10:37:45,617 - root - Save train data to data/open_images/sub-train-annotations-bbox.csv.\n",
      "2024-04-28 10:37:45,624 - root - Download https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-bbox.csv.\n",
      "2024-04-28 10:37:46,228 - root - Read annotation file data/open_images/validation-annotations-bbox.csv\n",
      "2024-04-28 10:37:46,351 - root - validation bounding boxes size: 50\n",
      "2024-04-28 10:37:46,351 - root - Approximate Image Stats: \n",
      "/Users/zhongyihao/Downloads/DS301 Advance ML/deep-learning/code/pytorch-ssd/open_images_downloader.py:63: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for k, count in values.value_counts().iteritems():\n",
      "2024-04-28 10:37:46,351 - root - Handgun: 20/39 = 0.51.\n",
      "2024-04-28 10:37:46,351 - root - Shotgun: 19/39 = 0.49.\n",
      "2024-04-28 10:37:46,351 - root - Label distribution: \n",
      "/Users/zhongyihao/Downloads/DS301 Advance ML/deep-learning/code/pytorch-ssd/open_images_downloader.py:63: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for k, count in values.value_counts().iteritems():\n",
      "2024-04-28 10:37:46,352 - root - Shotgun: 26/50 = 0.52.\n",
      "2024-04-28 10:37:46,352 - root - Handgun: 24/50 = 0.48.\n",
      "2024-04-28 10:37:46,352 - root - Shuffle dataset.\n",
      "2024-04-28 10:37:46,352 - root - Save validation data to data/open_images/sub-validation-annotations-bbox.csv.\n",
      "2024-04-28 10:37:46,353 - root - Download https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv.\n",
      "2024-04-28 10:37:47,615 - root - Read annotation file data/open_images/test-annotations-bbox.csv\n",
      "2024-04-28 10:37:47,967 - root - test bounding boxes size: 147\n",
      "2024-04-28 10:37:47,967 - root - Approximate Image Stats: \n",
      "/Users/zhongyihao/Downloads/DS301 Advance ML/deep-learning/code/pytorch-ssd/open_images_downloader.py:63: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for k, count in values.value_counts().iteritems():\n",
      "2024-04-28 10:37:47,967 - root - Handgun: 72/130 = 0.55.\n",
      "2024-04-28 10:37:47,967 - root - Shotgun: 58/130 = 0.45.\n",
      "2024-04-28 10:37:47,967 - root - Label distribution: \n",
      "/Users/zhongyihao/Downloads/DS301 Advance ML/deep-learning/code/pytorch-ssd/open_images_downloader.py:63: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for k, count in values.value_counts().iteritems():\n",
      "2024-04-28 10:37:47,967 - root - Handgun: 81/147 = 0.55.\n",
      "2024-04-28 10:37:47,967 - root - Shotgun: 66/147 = 0.45.\n",
      "2024-04-28 10:37:47,967 - root - Shuffle dataset.\n",
      "2024-04-28 10:37:47,967 - root - Save test data to data/open_images/sub-test-annotations-bbox.csv.\n",
      "2024-04-28 10:37:47,969 - root - Start downloading 1121 images.\n",
      "WARNING:root:Downloaded 100 images.\n",
      "WARNING:root:Downloaded 200 images.\n",
      "WARNING:root:Downloaded 300 images.\n",
      "WARNING:root:Downloaded 400 images.\n",
      "WARNING:root:Downloaded 500 images.\n",
      "WARNING:root:Downloaded 600 images.\n",
      "WARNING:root:Downloaded 700 images.\n",
      "WARNING:root:Downloaded 800 images.\n",
      "WARNING:root:Downloaded 900 images.\n",
      "WARNING:root:Downloaded 1000 images.\n",
      "WARNING:root:Downloaded 1100 images.\n",
      "2024-04-28 10:38:03,205 - root - Task Done.\n"
     ]
    }
   ],
   "source": [
    "!python pytorch-ssd/open_images_downloader.py --root data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download successfully, I choose Handguns and Shotguns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pytorch-ssd/train_ssd.py --dataset_type open_images --datasets data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001  --batch_size 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_nightly_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
